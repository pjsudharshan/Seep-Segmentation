{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Headers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from util import EarlyStopping, save_nets, save_predictions, load_best_weights\n",
    "from model import UNet\n",
    "from dataset import DataFolder\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hardcoded variables\n",
    "np. random.seed(1000)\n",
    "\n",
    "batch_size = 8 #change batch size here\n",
    "epochs = 200 #change number of epochs here\n",
    "lr = 0.001 \n",
    "patience = 10\n",
    "min_delta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Training, Validation, Test Sets\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=DataFolder('new_dataset/train/train_images_256/', 'new_dataset/train/train_masks_256/', 'train'),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "valid_loader = data.DataLoader(\n",
    "    dataset=DataFolder('new_dataset/val/train_images_256/', 'new_dataset/val/train_masks_256/', 'validation'),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    dataset=DataFolder('new_dataset/test/train_images_256/', 'new_dataset/test/train_masks_256/', 'test'),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet and other utilities construction\n",
    "\n",
    "model = UNet(1, shrink=1).cuda()\n",
    "nets = [model]\n",
    "params = [{'params': net.parameters()} for net in nets]\n",
    "solver = optim.Adam(params, lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "es = EarlyStopping(min_delta=min_delta, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Phase\n",
    "train_allepoch_loss = []\n",
    "valid_allepoch_loss = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    with tqdm(total=len(train_loader.dataset), desc=f'Epoch {epoch}/{epochs}', unit='img', position=0, leave=True) as pbar:\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "\n",
    "        for batch_idx, (img, mask, _) in enumerate(train_loader):\n",
    "\n",
    "            solver.zero_grad()\n",
    "\n",
    "            img = img.cuda()\n",
    "            mask = mask.cuda()\n",
    "\n",
    "            pred = model(img)\n",
    "            loss = criterion(pred, mask)\n",
    "            pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "            loss.backward()\n",
    "            solver.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            pbar.update(img.shape[0])\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (img, mask, _) in enumerate(valid_loader):\n",
    "\n",
    "                img = img.cuda()\n",
    "                mask = mask.cuda()\n",
    "\n",
    "                pred = model(img)\n",
    "                loss = criterion(pred, mask)\n",
    "\n",
    "                valid_loss.append(loss.item())\n",
    "                \n",
    "        train_loss_mean = np.mean(train_loss)\n",
    "        valid_loss_mean = np.mean(valid_loss)\n",
    "\n",
    "        print('[EPOCH {}/{}] Train Loss: {:.4f}; Valid Loss: {:.4f}'.format(\n",
    "            epoch, epochs, train_loss_mean, valid_loss_mean\n",
    "        ))\n",
    "\n",
    "        flag, best, bad_epochs = es.step(torch.Tensor([valid_loss_mean]))\n",
    "        if flag:\n",
    "            print('Early stopping criterion met')\n",
    "            break\n",
    "        else:\n",
    "            if bad_epochs == 0:\n",
    "                save_nets(nets, 'saved_model')\n",
    "                print('Saving current best model')\n",
    "\n",
    "            print('Current Valid loss: {:.4f}; Current best: {:.4f}; Bad epochs: {}'.format(\n",
    "                valid_loss_mean, best.item(), bad_epochs\n",
    "            ))\n",
    "    train_allepoch_loss.append(train_loss_mean)\n",
    "    valid_allepoch_loss.append(valid_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of training loss and validation loss across epochs\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1,len(train_allepoch_loss)+1), train_allepoch_loss, 'b-', label=\"Training Set Loss\")\n",
    "plt.plot(np.arange(1,len(valid_allepoch_loss)+1), valid_allepoch_loss, 'r-', label=\"Validation Set Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.savefig('saved_images/trainval_loss_n.svg',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "print('Training is over!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Phase\n",
    "\n",
    "with torch.no_grad():\n",
    "        test_loss = []\n",
    "        for batch_idx, (img, mask, img_fns) in enumerate(test_loader):\n",
    "    \n",
    "            model = load_best_weights(model, 'saved_model')\n",
    "    \n",
    "            img = img.cuda()\n",
    "            mask = mask.cuda()\n",
    "    \n",
    "            pred = model(img)\n",
    "            loss = criterion(pred, mask)\n",
    "    \n",
    "            test_loss.append(loss.item())\n",
    "            \n",
    "            pred_mask = torch.argmax(F.softmax(pred, dim=1), dim=1)\n",
    "            pred_mask = torch.chunk(pred_mask, chunks=batch_size, dim=0)\n",
    "            save_predictions(pred_mask, img_fns, 'test_predictions')\n",
    "    \n",
    "            print('[Tested: {}/{}] Test Loss: {:.4f}'.format(\n",
    "                batch_idx+1, len(test_loader), loss.item()\n",
    "            ))\n",
    "    \n",
    "print('Final Test Loss: {:.4f}'.format(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of last batch of test set\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "cmap = mpl.colors.ListedColormap(['black','blue','red', 'green', 'brown', 'cyan','yellow','royalblue'])\n",
    "cmap.set_over('royalblue')\n",
    "cmap.set_under('black')\n",
    "bounds = [0,1,2,3,4,5,6,7,8]\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "for idx, pred in enumerate(pred_mask):\n",
    "    pred = torch.squeeze(pred, dim=0)\n",
    "    ax = fig.add_subplot(3,3, idx+1)\n",
    "    img = ax.imshow(pred.cpu().numpy(), interpolation='none', cmap=cmap, norm=norm)\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(img)\n",
    "plt.savefig('saved_images/predictions_n.svg',transparent=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
